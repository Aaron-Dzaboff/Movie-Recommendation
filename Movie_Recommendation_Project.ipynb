{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Recommendation Project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjvH_0ECUPeu",
        "outputId": "aa6f8e68-662a-4874-a165-d8580264d848"
      },
      "source": [
        "#Importing all the data\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnD-3zxwUV9j",
        "outputId": "ed13e90b-b520-431a-e645-66bf8c7a6444"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Movie Recommendation Data\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ratings.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NHj-imuUYry"
      },
      "source": [
        "#Setting up Spark environment\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENL28CIYnDQ"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Movie').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIa88DxKYpXf",
        "outputId": "a4be5b21-d77f-4b87-e72b-8ec05d7b583f"
      },
      "source": [
        "df = spark.read.csv('/content/drive/My Drive/Movie Recommendation Data/ratings.csv', header=True, sep=',', inferSchema=True)\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+------+----------+\n",
            "|userId|movieId|rating| timestamp|\n",
            "+------+-------+------+----------+\n",
            "|     1|    296|   5.0|1147880044|\n",
            "|     1|    306|   3.5|1147868817|\n",
            "|     1|    307|   5.0|1147868828|\n",
            "+------+-------+------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLY5QftDgTs4",
        "outputId": "4e254599-2e9e-4886-fd07-1c7dfaafa25d"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovomYCjzZy5i",
        "outputId": "3c22e099-8e76-40c4-9917-37045bef453a"
      },
      "source": [
        "df = df.drop(df['timestamp'])\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+------+\n",
            "|userId|movieId|rating|\n",
            "+------+-------+------+\n",
            "|     1|    296|   5.0|\n",
            "|     1|    306|   3.5|\n",
            "|     1|    307|   5.0|\n",
            "+------+-------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG8VCo7qcRvN",
        "outputId": "581174b4-2d10-4ae2-c7cb-b8e15a5a7ced"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----------------+------------------+------------------+\n",
            "|summary|           userId|           movieId|            rating|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "|  count|         25000095|          25000095|          25000095|\n",
            "|   mean|81189.28115381162|21387.981943268616| 3.533854451353085|\n",
            "| stddev|46791.71589745776| 39198.86210105973|1.0607439611423535|\n",
            "|    min|                1|                 1|               0.5|\n",
            "|    max|           162541|            209171|               5.0|\n",
            "+-------+-----------------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6c1iH81cidM"
      },
      "source": [
        "#Splitting the dataset\r\n",
        "(training, test) = df.randomSplit([0.8, 0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSDiTZXOd2ts"
      },
      "source": [
        "# Build the recommendation model using ALS on the training data\r\n",
        "from pyspark.ml.recommendation import ALS\r\n",
        "als = ALS(maxIter=10, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\r\n",
        "model = als.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GcthLMfnKB",
        "outputId": "4208e325-db1c-4cb3-ef85-f3bc5994322e"
      },
      "source": [
        "predictions = model.transform(test)\r\n",
        "predictions.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+------+----------+\n",
            "|userId|movieId|rating|prediction|\n",
            "+------+-------+------+----------+\n",
            "| 32855|    148|   4.0| 2.3492625|\n",
            "|114572|    148|   2.0| 2.7763708|\n",
            "| 38199|    148|   2.0| 2.5441282|\n",
            "| 33354|    148|   3.0| 3.1871886|\n",
            "|  5055|    148|   3.0| 3.3531446|\n",
            "| 99010|    148|   3.0| 2.9363954|\n",
            "| 38679|    148|   3.0|  2.598993|\n",
            "| 99684|    148|   3.0| 2.9181907|\n",
            "| 35969|    148|   2.0| 2.1641223|\n",
            "| 29943|    148|   3.0| 2.9938407|\n",
            "+------+-------+------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWxFf4cigM0o",
        "outputId": "6aee53f3-facf-4385-b81e-a3798b84f97b"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\r\n",
        "rmse = evaluator.evaluate(predictions)\r\n",
        "print(\"Root-mean-square error = \" + str(rmse))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root-mean-square error = nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9eeqN62gpw0",
        "outputId": "d50fa45f-ace8-4ade-d6ec-f0dcf76940bd"
      },
      "source": [
        "single_user = test.filter(test['userId']==8).select(['movieId','userId','rating'])\r\n",
        "# User had 10 ratings in the test data set \r\n",
        "# Realistically this should be some sort of hold out set!\r\n",
        "single_user.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+------+\n",
            "|movieId|userId|rating|\n",
            "+-------+------+------+\n",
            "|     47|     8|   5.0|\n",
            "|    104|     8|   5.0|\n",
            "|    110|     8|   5.0|\n",
            "|    161|     8|   5.0|\n",
            "|    209|     8|   4.0|\n",
            "|    223|     8|   2.0|\n",
            "|    225|     8|   2.0|\n",
            "|    237|     8|   4.0|\n",
            "|    260|     8|   3.0|\n",
            "|    344|     8|   5.0|\n",
            "|    364|     8|   4.0|\n",
            "|    377|     8|   5.0|\n",
            "|    474|     8|   4.0|\n",
            "|    524|     8|   4.0|\n",
            "|    590|     8|   3.0|\n",
            "|    593|     8|   4.0|\n",
            "|    595|     8|   2.0|\n",
            "|    799|     8|   4.0|\n",
            "|    953|     8|   4.0|\n",
            "|   1020|     8|   4.0|\n",
            "+-------+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwL_WB4Iiv_Z",
        "outputId": "7f869c2f-dfc0-4a9a-b4de-88c97c30add4"
      },
      "source": [
        "from pyspark.sql.functions import desc\r\n",
        "single_user_predictions = model.transform(single_user)\r\n",
        "single_user_predictions.orderBy('prediction', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+------+----------+\n",
            "|movieId|userId|rating|prediction|\n",
            "+-------+------+------+----------+\n",
            "|    344|     8|   5.0| 4.7764993|\n",
            "|    104|     8|   5.0| 4.6258297|\n",
            "|     47|     8|   5.0| 4.3787174|\n",
            "|    377|     8|   5.0|   4.03852|\n",
            "|   1729|     8|   5.0| 3.8794904|\n",
            "|    593|     8|   4.0| 3.8407557|\n",
            "|    223|     8|   2.0| 3.7758756|\n",
            "|   1385|     8|   5.0| 3.7702544|\n",
            "|    364|     8|   4.0| 3.7370305|\n",
            "|   1580|     8|   5.0| 3.7041347|\n",
            "|    225|     8|   2.0| 3.5131419|\n",
            "|    474|     8|   4.0| 3.5103621|\n",
            "|   1653|     8|   2.0| 3.4900517|\n",
            "|    110|     8|   5.0| 3.4288478|\n",
            "|    237|     8|   4.0| 3.4210522|\n",
            "|   1722|     8|   3.0| 3.4041586|\n",
            "|    161|     8|   5.0|  3.361381|\n",
            "|    953|     8|   4.0|  3.327233|\n",
            "|   1407|     8|   5.0|  3.312661|\n",
            "|   1597|     8|   4.0| 3.3049188|\n",
            "+-------+------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj7EP_ziPpMy",
        "outputId": "41a7efb0-35f3-45fc-aa23-54199e51fa27"
      },
      "source": [
        "single_user = test.filter(test['userId']==23).select(['movieId','userId','rating'])\r\n",
        "# User had 10 ratings in the test data set \r\n",
        "# Realistically this should be some sort of hold out set!\r\n",
        "single_user.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+------+\n",
            "|movieId|userId|rating|\n",
            "+-------+------+------+\n",
            "|     16|    23|   4.0|\n",
            "|     25|    23|   4.0|\n",
            "|     36|    23|   5.0|\n",
            "|     39|    23|   3.0|\n",
            "|     40|    23|   4.0|\n",
            "|    165|    23|   3.0|\n",
            "|    216|    23|   4.0|\n",
            "|    232|    23|   4.0|\n",
            "|    260|    23|   5.0|\n",
            "|    281|    23|   4.0|\n",
            "|    319|    23|   4.0|\n",
            "|    369|    23|   4.0|\n",
            "|    380|    23|   3.0|\n",
            "|    440|    23|   4.0|\n",
            "|    491|    23|   5.0|\n",
            "|    529|    23|   4.0|\n",
            "|    541|    23|   4.0|\n",
            "|    608|    23|   4.0|\n",
            "|    648|    23|   3.0|\n",
            "|    733|    23|   4.0|\n",
            "+-------+------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAYGZqs9Pt3K",
        "outputId": "d2d529b1-034a-4050-e9ab-1491073a87f4"
      },
      "source": [
        "from pyspark.sql.functions import desc\r\n",
        "single_user_predictions = model.transform(single_user)\r\n",
        "single_user_predictions.orderBy('prediction', ascending=False).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+------+----------+\n",
            "|movieId|userId|rating|prediction|\n",
            "+-------+------+------+----------+\n",
            "|   1207|    23|   5.0| 4.9482827|\n",
            "|   1704|    23|   5.0|  4.814985|\n",
            "|   1132|    23|   3.0| 4.7642217|\n",
            "|   1293|    23|   5.0| 4.7327785|\n",
            "|   1252|    23|   5.0|  4.705429|\n",
            "|   1784|    23|   4.0|   4.56531|\n",
            "|   1208|    23|   5.0| 4.5606303|\n",
            "|   1231|    23|   5.0|  4.535055|\n",
            "|   2918|    23|   5.0| 4.5329037|\n",
            "|    232|    23|   4.0| 4.5252237|\n",
            "|   1228|    23|   5.0| 4.5129967|\n",
            "|    608|    23|   4.0|  4.454386|\n",
            "|    260|    23|   5.0|  4.446644|\n",
            "|   1196|    23|   5.0| 4.4387074|\n",
            "|   2571|    23|   1.0| 4.4121437|\n",
            "|   1834|    23|   4.0| 4.3869414|\n",
            "|    529|    23|   4.0|  4.367726|\n",
            "|    281|    23|   4.0|  4.345355|\n",
            "|   1968|    23|   5.0|  4.339189|\n",
            "|   2352|    23|   5.0| 4.3029094|\n",
            "+-------+------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNrikqN9Z7aL",
        "outputId": "82be2bb4-e771-4e70-9bb4-28d98cb6c390"
      },
      "source": [
        "#Creating a feature vector and then one-hot encode the movie ids\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "encoder = OneHotEncoderEstimator(inputCols=['movieId'], outputCols=['movie_vec'], dropLast=False)  #by putting droplast to false we will have a n dimensional vector and not a n-1 dimensional feature vector\n",
        "dfvec = encoder.fit(df).transform(df) \n",
        "dfvec.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------+------+--------------------+\n",
            "|userId|movieId|rating|           movie_vec|\n",
            "+------+-------+------+--------------------+\n",
            "|     1|    296|   5.0|(209172,[296],[1.0])|\n",
            "|     1|    306|   3.5|(209172,[306],[1.0])|\n",
            "|     1|    307|   5.0|(209172,[307],[1.0])|\n",
            "|     1|    665|   5.0|(209172,[665],[1.0])|\n",
            "|     1|    899|   3.5|(209172,[899],[1.0])|\n",
            "|     1|   1088|   4.0|(209172,[1088],[1...|\n",
            "|     1|   1175|   3.5|(209172,[1175],[1...|\n",
            "|     1|   1217|   3.5|(209172,[1217],[1...|\n",
            "|     1|   1237|   5.0|(209172,[1237],[1...|\n",
            "|     1|   1250|   4.0|(209172,[1250],[1...|\n",
            "|     1|   1260|   3.5|(209172,[1260],[1...|\n",
            "|     1|   1653|   4.0|(209172,[1653],[1...|\n",
            "|     1|   2011|   2.5|(209172,[2011],[1...|\n",
            "|     1|   2012|   2.5|(209172,[2012],[1...|\n",
            "|     1|   2068|   2.5|(209172,[2068],[1...|\n",
            "|     1|   2161|   3.5|(209172,[2161],[1...|\n",
            "|     1|   2351|   4.5|(209172,[2351],[1...|\n",
            "|     1|   2573|   4.0|(209172,[2573],[1...|\n",
            "|     1|   2632|   5.0|(209172,[2632],[1...|\n",
            "|     1|   2692|   5.0|(209172,[2692],[1...|\n",
            "+------+-------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIORfxHIggtW",
        "outputId": "bec0d83b-52f0-49ab-e81d-3ba0d15beda6"
      },
      "source": [
        "dfvec.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTrg4pVndK3L"
      },
      "source": [
        "## Going to try to find similarities b/w users \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80avPJrvdQYL"
      },
      "source": [
        "Users have rated more than one movie. So we need to some how aggregate the feature vector (movie_vec) in a way that movies are grouped together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC178Ni_dBsL",
        "outputId": "a957b34b-966b-45fb-90ff-0da1a0d61ae6"
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
        "\n",
        "def vector_sum(vector_list):\n",
        "  indices = []\n",
        "  values = []\n",
        "  for v in vector_list:\n",
        "    indices.append(v.indices)\n",
        "    values.append(v.values)\n",
        "  indices.sort()\n",
        "  return SparseVector(208940, indices, values)\n",
        "\n",
        "sum_udf = udf(vector_sum, VectorUDT())\n",
        "\n",
        "df_agg = dfvec.groupby('userId').agg(F.collect_list('movie_vec').alias('movie_vec')).withColumn('movie_vec', sum_udf('movie_vec'))\n",
        "df_agg.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+\n",
            "|userId|           movie_vec|\n",
            "+------+--------------------+\n",
            "|   148|(208940,[19,32,47...|\n",
            "|   463|(208940,[3,6,7,32...|\n",
            "|   471|(208940,[318,356,...|\n",
            "+------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcOJAXcAd5QT",
        "outputId": "17837635-bfdf-4782-eef3-e01918a248f3"
      },
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "lsh = MinHashLSH(inputCol = 'movie_vec', outputCol='hashes', numHashTables=3)\n",
        "lsh_model = lsh.fit(df_agg)\n",
        "lsh_model.transform(df_agg).show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------------+\n",
            "|userId|           movie_vec|              hashes|\n",
            "+------+--------------------+--------------------+\n",
            "|   148|(208940,[19,32,47...|[[3.9422642E7], [...|\n",
            "|   463|(208940,[3,6,7,32...|[[4.6905538E7], [...|\n",
            "|   471|(208940,[318,356,...|[[1.1399122E7], [...|\n",
            "+------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBcgevmphRdg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68e0196b-e860-4838-db91-25d308a42606"
      },
      "source": [
        "lsh_model.approxSimilarityJoin(df_agg, df_agg, 0.6).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c12a6170fd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsh_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxSimilarityJoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_agg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o377.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 141.0 failed 1 times, most recent failure: Lost task 3.0 in stage 141.0 (TID 1016, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-22-d4bcab63728d>\", line 12, in vector_sum\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 532, in __init__\n    % (np.max(self.indices), self.size)\nAssertionError: Index 209151 is out of the size of vector with size=208940\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-22-d4bcab63728d>\", line 12, in vector_sum\n  File \"/content/spark-2.4.7-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/ml/linalg/__init__.py\", line 532, in __init__\n    % (np.max(self.indices), self.size)\nAssertionError: Index 209151 is out of the size of vector with size=208940\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCxDHU67ilE8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}